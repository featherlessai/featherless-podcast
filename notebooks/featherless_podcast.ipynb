{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featherless.ai - PDF to Podcast Pipeline\n",
    "This notebook demonstrates a complete workflow for:\n",
    "1. **Extracting content** from PDF documents using PyMuPDF\n",
    "2. **Processing and cleaning** the extracted text with [Featherless.ai](http://featherless.ai) API\n",
    "3. **Generating podcast-style content** from the processed text\n",
    "\n",
    "The pipeline handles:\n",
    "- PDF validation and extraction\n",
    "- Text chunking for efficient processing\n",
    "- API calls to Featherless AI for text cleaning\n",
    "- Output management and preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we need to install required libraries and set up our configuration.\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "- `pdf_path`: Path to the PDF file to be processed\n",
    "- `DEFAULT_MODEL`: The Featherless AI model to use for text processing\n",
    "- `BASE_URL`: The Featherless API endpoint\n",
    "- `FEATHERLESS_API_KEY`: Your API key for authentication\n",
    "- `CHUNK_SIZE`: The target size for text chunks (in characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install rich ipywidgets pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '/path_to_your_pdf.pdf'\n",
    "DEFAULT_MODEL = \"Qwen/Qwen2.5-72B-Instruct\" # Go through our model catalog on https://featherless.ai/models\n",
    "# Configuration\n",
    "BASE_URL = \"https://api.featherless.ai/v1\"\n",
    "FEATHERLESS_API_KEY = \"YOUR_FEATHERLESS_API_KEY\" # Available in https://featherless.ai/account/api-keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Handling Functions\n",
    "\n",
    "These functions handle PDF validation, metadata extraction, and text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 60000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to markdown text\n",
    "        markdown_text = pymupdf4llm.to_markdown(file_path)\n",
    "        \n",
    "        # Truncate if exceeds max_chars\n",
    "        if len(markdown_text) > max_chars:\n",
    "            print(f\"Truncating text to {max_chars} characters\")\n",
    "            markdown_text = markdown_text[:max_chars]\n",
    "        \n",
    "        print(f\"\\nExtraction complete! Total characters: {len(markdown_text)}\")\n",
    "        return markdown_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get metadata using page chunks feature\n",
    "        data = pymupdf4llm.to_markdown(file_path, page_chunks=True)\n",
    "        \n",
    "        metadata = {\n",
    "            'num_pages': len(data),\n",
    "            'metadata': {\n",
    "                'pages': [page.get('metadata', {}) for page in data]\n",
    "            }\n",
    "        }\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Configuration\n",
    "\n",
    "Setting up the system prompt for the Featherless AI model to clean the PDF text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking Functions\n",
    "\n",
    "The following functions split the extracted text into manageable chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input text to be split into chunks\n",
    "    target_chunk_size : int\n",
    "        Approximate size for each chunk in characters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of str\n",
    "        Text split into chunks that don't break words\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Call Function\n",
    "\n",
    "This function sends each text chunk to the Featherless AI API for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text using Featherless API\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {FEATHERLESS_API_KEY}\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": DEFAULT_MODEL,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        processed_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Print chunk information for monitoring\n",
    "        print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "        print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "        print(f\"{'='*90}\\n\")\n",
    "        \n",
    "        return processed_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
    "        return text_chunk  # Return original text in case of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Workflow\n",
    "\n",
    "The following cells implement the complete workflow:\n",
    "1. Read the file\n",
    "2. Create chunks\n",
    "3. Process each chunk\n",
    "4. Save and preview the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"./extracted_text.txt\"  # Replace with your file path\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "# Read the file content first\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text_content = file.read()\n",
    "\n",
    "# Create chunks from the actual content\n",
    "chunks = create_word_bounded_chunks(text_content, CHUNK_SIZE)\n",
    "num_chunks = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, initialize the text variable\n",
    "processed_text = \"\"\n",
    "# Read the file\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Create output file name\n",
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        \n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total chunks processed: {num_chunks}\")\n",
    "\n",
    "# Preview the beginning and end of the complete processed text\n",
    "print(\"\\nPreview of final processed text:\")\n",
    "print(\"\\nBEGINNING:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nEND:\")\n",
    "print(processed_text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate text statistics\n",
    "original_length = len(text)\n",
    "processed_length = len(processed_text)\n",
    "reduction_percent = ((original_length - processed_length) / original_length) * 100\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(['Original Text', 'Processed Text'], \n",
    "        [original_length, processed_length],\n",
    "        color=['#1f77b4', '#2ca02c'])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Character Count')\n",
    "ax.set_title('Text Processing Results')\n",
    "\n",
    "# Add text on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:,}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Add reduction percentage\n",
    "plt.figtext(0.5, 0.01, f\"Text reduction: {reduction_percent:.1f}%\", \n",
    "           ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
